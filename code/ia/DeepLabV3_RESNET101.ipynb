{"cells":[{"cell_type":"markdown","metadata":{"id":"yvvEPZP_zwa6"},"source":["**Possibilitando acesso a drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHM8VHCjUGLR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d90febb-7a3f-4bd2-c93b-35c35ae39792","executionInfo":{"status":"ok","timestamp":1668088891996,"user_tz":180,"elapsed":27243,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import data_table\n","data_table.enable_dataframe_formatter()"],"metadata":{"id":"-4dBY6sUFX7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nabwFGiz5w5"},"source":["**Importando bibliotecas**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7C4xX0NUzmL"},"outputs":[],"source":["import os \n","import time\n","import numpy as np \n","import cv2 \n","import torchvision.models.segmentation \n","import torch \n","import torchvision.transforms as tf\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","#import segmentation_models_pytorch as smp\n","import torch.nn as nn\n","import time\n","import pandas as pd\n","import random\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","# Segmentation Model\n","#from torchvision.io.image import read_image\n","#from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n","#from torchvision.transforms.functional import to_pil_image\n","\n","# Função de Recall\n","from sklearn.metrics import recall_score\n","# Função de F1\n","from sklearn.metrics import f1_score\n","# Função de Precision\n","from sklearn.metrics import precision_score\n","# Função de Accuracy\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"Kp9Me-seaoqY"},"source":["**Inicializando constantes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqFtRtwoIZ8T"},"outputs":[],"source":["model_name = 'DeepLabV3_RESNET101'\n","model_folder = f'/content/drive/MyDrive/DEV/saved-weights/{model_name}/'\n","\n","ImagesFolder=\"/content/drive/MyDrive/DEV/corte2/\" # ALTERAR ESSE CAMINHO PARA A PASTA CORTE 2 NA NUVEM COM GPUb"]},{"cell_type":"code","source":["n_epochs = 100\n","batchSize = 2 # Esse valor precisa ser divisor do número de imagens no treino\n","batchSize_val = 2 # Esse valor precisa ser divisor do número de imagens na validação\n","Learning_Rate = 1e-5"],"metadata":{"id":"iqvCNFiNmKMn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HY8LXMuAasic"},"source":["**Carregando arquivo com dataframe do dataset \"aleatoriezado\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CwKoSvj_F4L4","outputId":"03891b67-94ba-4fbd-9dda-b029fd41fe37","executionInfo":{"status":"ok","timestamp":1668088944765,"user_tz":180,"elapsed":746,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Train    640\n","Test      78\n","Val       78\n","Name: Status, dtype: int64"]},"metadata":{},"execution_count":6}],"source":["# Carregando dataframe de um arquivo chamado \"all_dataset_dataframe.csv\"\n","# Referencia https://towardsdatascience.com/how-to-read-csv-file-using-pandas-ab1f5e7e7b58\n","df = pd.read_csv(f'{ImagesFolder}all_dataset_dataframe.csv') # ALTERAR ESSE CAMINHO PARA A LOCALIZAÇÃO DO DATAFRAME NA NUVEM COM GPU\n","df['Status'].value_counts()\n"]},{"cell_type":"markdown","metadata":{"id":"ywxPjkHJbvi_"},"source":["**Lendo valores de desvio padrão e média de um arquivo**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPZIgdYkmREx","outputId":"4368555a-2ecb-4a0d-bea6-1454b68ffa32","executionInfo":{"status":"ok","timestamp":1668088950069,"user_tz":180,"elapsed":1390,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6604449458815983, 0.6988043731738397, 0.6452534234104293]\n","[0.1753238443181036, 0.19331901534376825, 0.1546044630633522]\n"]}],"source":["# Lendo a média e desvio padrão do arquivo \"mean_and_std_list.txt\"\n","mean_and_std_list_file = open(f\"{ImagesFolder}mean_and_std.txt\", \"r\")\n","content_list = mean_and_std_list_file.readlines()\n","\n","mean = [float(integer) for integer in content_list[1].split(' ')[0:3]]\n","std = [float(integer) for integer in content_list[3].split(' ')[0:3]]\n","\n","print(mean)\n","print(std)"]},{"cell_type":"markdown","metadata":{"id":"qnydFlHET-QS"},"source":["**Declaração do Dataset pelo Pytorch**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjrWa2An7FgK"},"outputs":[],"source":["# Referência: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","class CustomImageDataset(Dataset):\n","    \n","  def __init__(self, df):\n","    self.dataframe = df\n","    #----------------------------------------------Funções de transformação-------------------------------------------------------------------#\n","    self.transformImg=tf.Compose([tf.ToPILImage(), tf.ToTensor(),tf.Normalize((mean[0], mean[1], mean[2]), (std[0], std[1], std[2]))])\n","    self.transformMask=tf.Compose([tf.ToPILImage(), tf.ToTensor()])\n","\n","  def __len__(self):\n","    return len(self.dataframe)\n","\n","  def __getitem__(self, idx):\n","    pair_path = os.path.join(ImagesFolder,  self.dataframe.iloc[idx].Path)\n","    Img = cv2.imread(os.path.join(pair_path, \"img.png\"))\n","    Label = cv2.imread(os.path.join(pair_path, \"label.png\"),0)\n","\n","    mask = np.zeros(Img.shape[0:2],np.float32)\n","    mask[Label != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","\n","    image = self.transformImg(Img)\n","    label = self.transformMask(mask)\n","    return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrUgP-UOOQFf"},"outputs":[],"source":["ds_train=CustomImageDataset(df[df['Status'] == 'Train'])\n","ds_val=CustomImageDataset(df[df['Status'] == 'Val'])"]},{"cell_type":"code","source":["print(len(ds_train))\n","print(len(ds_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fim9P8bay_Ga","outputId":"a92a09b3-548c-47eb-e49a-a1af7204a4a2","executionInfo":{"status":"ok","timestamp":1668088963088,"user_tz":180,"elapsed":797,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["640\n","78\n"]}]},{"cell_type":"markdown","metadata":{"id":"4Fa0qK2VULgJ"},"source":["**Instanciação do DataLoader's pelo Pytorch**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xi_buGBPQ1Jd"},"outputs":[],"source":["train_dataloader = DataLoader(ds_train, batch_size=batchSize, shuffle=True)\n","val_dataloader = DataLoader(ds_val, batch_size=batchSize_val, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"GK6z-HMjUclc"},"source":[" **Definição da estrutura DeepLabV3_RESNET101**\n"]},{"cell_type":"markdown","metadata":{"id":"vsaE6vAic3wN"},"source":[" Referência: https://pytorch.org/vision/stable/models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html"]},{"cell_type":"markdown","source":["# MODEL"],"metadata":{"id":"0PLB8GVLNg7d"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["8a6b7fdc261c4df196aaca3623fe72e6","e4943ada50864053a682a79df52e53fb","8ca0b46d18f4491fae74df37f9aaf71c","48b55f64b3e74f789faca6379fa7a888","bbfde3b227014b2c8569183d938fb50b","f719704deca24cd29f630e89681ef27e","0d7b8d95a3fc456bb70a5164c3432404","b4a5119f0dca474b9add5d82bd1ab5fa","90de15d08366464893e71b806bce0584","806e19abcc234c44a10347b765797aa2","b6f162f89e814469a253e338d32cfd0e"]},"id":"_AwKg3JNPVX5","outputId":"66fb53e2-3817-4867-f366-e2e46047889d","executionInfo":{"status":"ok","timestamp":1668088981618,"user_tz":180,"elapsed":2566,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/233M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a6b7fdc261c4df196aaca3623fe72e6"}},"metadata":{}}],"source":["#--------------Load and set net and optimizer-------------------------------------\n","Net = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True) # Load net\n","Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 2 classes"]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","Net=Net.to(device)\n","optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"],"metadata":{"id":"dfbE59OiNzRV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftzhro928ZzG"},"source":["**Treinamento**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxHqvnR-P5-A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f35bcd7-a96a-4459-804c-0aa8552227d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["training epoch: 0\n","it = 0, training_loss = 51.583883725106716, val_loss = 5.125071927905083, delta =  1.0\n","training epoch: 1\n","it = 1, training_loss = 37.26162952929735, val_loss = 4.0210626274347305, delta =  0.2154134256066188\n","training epoch: 2\n","it = 2, training_loss = 29.317352056503296, val_loss = 3.3123425021767616, delta =  0.176251949030225\n","training epoch: 3\n","it = 3, training_loss = 23.694048650562763, val_loss = 2.689329367130995, delta =  0.18808837994148953\n","training epoch: 4\n","it = 4, training_loss = 19.437508013099432, val_loss = 2.3169526644051075, delta =  0.13846452103527307\n","training epoch: 5\n","it = 5, training_loss = 16.261456571519375, val_loss = 2.0400192961096764, delta =  0.11952482782661233\n","training epoch: 6\n","it = 6, training_loss = 13.666549298912287, val_loss = 1.7006996348500252, delta =  0.16633159397400554\n","training epoch: 7\n","it = 7, training_loss = 11.636087400838733, val_loss = 1.5400671027600765, delta =  0.09445085351836036\n","training epoch: 8\n","it = 8, training_loss = 9.965473182499409, val_loss = 1.37569884583354, delta =  0.10672798388587035\n","training epoch: 9\n","it = 9, training_loss = 8.553183229640126, val_loss = 1.2731650471687317, delta =  0.07453215431221993\n","training epoch: 10\n","it = 10, training_loss = 7.538545647636056, val_loss = 1.364396594464779, delta =  -0.07165728237586189\n","training epoch: 11\n","it = 11, training_loss = 6.890882734209299, val_loss = 1.11390190012753, delta =  0.12509230236517366\n","training epoch: 12\n","it = 12, training_loss = 5.853692068718374, val_loss = 0.9919781256467104, delta =  0.1094564740996139\n","training epoch: 13\n","it = 13, training_loss = 5.047861692495644, val_loss = 1.0793037666007876, delta =  -0.08803182116253438\n","training epoch: 14\n","it = 14, training_loss = 4.407274404540658, val_loss = 0.9699242068454623, delta =  0.022232263223415538\n","training epoch: 15\n","it = 15, training_loss = 3.917802774347365, val_loss = 0.9482740657404065, delta =  0.022321477237350118\n","training epoch: 16\n","it = 16, training_loss = 3.522202509455383, val_loss = 0.9497840125113726, delta =  -0.001592310520257767\n","training epoch: 17\n","it = 17, training_loss = 3.152953122742474, val_loss = 1.0941912103444338, delta =  -0.15387655307234005\n","training epoch: 18\n","it = 18, training_loss = 2.847763169556856, val_loss = 0.8695933958515525, delta =  0.08297249996752853\n","training epoch: 19\n","it = 19, training_loss = 2.5882856817916036, val_loss = 0.7990171732380986, delta =  0.08116002599622074\n","training epoch: 20\n"]}],"source":["epoch = []\n","\n","train_loss = []\n","train_loss_sum = 0\n","\n","val_loss = []\n","val_loss_sum = 0\n","\n","cont2 = 0\n","\n","val_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n","               'f1_macro' : [], 'f1_micro' : [], 'training_loss' : [], 'validation_loss' : []}\n","loss_dict = {'training' : [], 'validation' : []} \n","\n","train_prev_loss = float('-inf')\n","train_best_loss = float('inf')\n","train_last_improvement = 0\n","\n","\n","val_prev_loss = float('-inf')\n","val_best_loss = float('inf')\n","val_last_improvement = 0\n","\n","patience_n_iterations = 5\n","patience_min_threshold = 0.01\n","\n","\n","#Net.load_state_dict(torch.load(\"0.torch\")) # Load trained model\n","start = time.time()\n","for itr in range(n_epochs): # Training loop\n","  print(f'training epoch: {itr}')\n","  batch_it = 0\n","  train_loss_sum = 0\n","  \n","  #TRAINING\n","  Net.train()\n","  for images, mask in train_dataloader:\n","    Net.zero_grad()\n","    \n","    # Iterar sobre o dataloader do treino    \n","    mask = mask.squeeze()\n","    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    \n","    Pred=Net(images)['out'] # make prediction\n","    \n","    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n","    \n","    Loss = criterion(Pred,mask.long()) # Calculate cross entropy loss    \n","    Loss.backward() # Backpropogate loss\n","    optimizer.step() # Apply gradient descent change to weight\n","    \n","    #print(f'Training Batch Iteration = {batch_it}, Training Loss = {Loss.data.cpu().numpy()}')\n","    train_loss_sum += Loss.data.cpu().numpy()\n","    batch_it += 1\n","    \n","  delta = 1 - train_loss_sum / train_prev_loss\n","  if(delta >= patience_min_threshold):\n","    train_prev_loss = train_loss_sum\n","    train_last_improvement = 0  \n","  else:\n","    train_last_improvement += 1\n","    if train_last_improvement >= patience_n_iterations:\n","        break\n","  \n","  train_loss.append(train_loss_sum/len(train_dataloader))\n","\n","  #VALIDATION \n","  val_loss_sum = 0\n","\n","  Net.eval()\n","  with torch.no_grad():\n","        \n","    for images, mask in val_dataloader:\n","        \n","      mask = mask.squeeze()\n","      AnnMap = np.zeros(mask.shape,np.float32)\n","      AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","    \n","      images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","      mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    \n","      Pred = Net(images)['out'] # make prediction     \n","\n","      criterion = torch.nn.CrossEntropyLoss() # Set loss function\n","      Loss=criterion(Pred,mask.long()) # Calculate cross entropy loss      \n","    \n","      val_loss_sum += Loss.data.cpu().numpy()\n","      cont2+=1\n","\n","\n","  delta = 1 - val_loss_sum / val_prev_loss\n","  if(delta >= patience_min_threshold):\n","    val_prev_loss = val_loss_sum\n","    val_last_improvement = 0  \n","  else:\n","    val_last_improvement += 1\n","    if val_last_improvement >= patience_n_iterations:\n","        break  \n","\n","  if(val_loss_sum < val_best_loss ):\n","    val_best_loss  = val_loss_sum\n","    model_weight_name = f'{model_name}_best_{Learning_Rate}'\n","    torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n","  \n","  val_loss.append(val_loss_sum/len(val_dataloader))\n","\n","  # Salvando perdas em dataframe\n","  loss_dict['training'].append(train_loss_sum)\n","  loss_dict['validation'].append(val_loss_sum)\n","\n","  # Salvando pesos\n","  model_weight_name = f'{model_name}_{Learning_Rate}'  \n","  torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n","\n","  print(f'it = {itr}, training_loss = {train_loss_sum}, val_loss = {val_loss_sum}, delta =  {delta}')\n","# ---- FINALIZANDO TREINO ----- #\n","end = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2V0kyvcxY2h"},"outputs":[],"source":["# Salvando dataframe de métricas\n","df_metrics = pd.DataFrame(val_metrics)\n","df_metrics.to_csv('dataframe_metrics.csv', index=False)\n","\n","# Duração do treino e validação\n","trainingDuration = time.strftime(\"%H:%M:%S\", time.gmtime((end-start)))\n","print(f\"{trainingDuration} com {n_epochs} épocas\")\n","\n","# Plot de gráfico loss do treino por épocas de treinamento\n","plt.title(\"Model Loss\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","\n","for i in range(n_epochs):\n","  plt.plot(train_loss, color=\"b\", label=\"train\")\n","  plt.plot(val_loss, color=\"g\", label=\"validation\")\n","  plt.legend([\"train\",\"validation\"])"]},{"cell_type":"markdown","source":["# TESTING"],"metadata":{"id":"VxIwXQq45_4C"}},{"cell_type":"code","source":["model_path = f'{model_folder}{model_name}_best_{Learning_Rate}.torch'\n","\n","Net = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)  # Load net\n","Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))  # Change final layer to 3 classes\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","Net = Net.to(device)  # Set net to GPU or CPU\n","Net.load_state_dict(torch.load(model_path,  map_location=device)) # Load trained model\n","Net.eval() # Set to evaluation mode"],"metadata":{"id":"68ci_Fiwz0sB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataloader = DataLoader(CustomImageDataset(df[df['Status'] == 'Test']), batch_size=1, shuffle=False)"],"metadata":{"id":"hEH5xMxO6Cfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n","               'f1_macro' : [], 'f1_micro' : []}"],"metadata":{"id":"WuaaRDqPo7ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Net.eval()\n","with torch.no_grad():\n","  # Iterar sobre o dataloader da validação, configurar o batchsize por questões de memória\n","  for images, mask in test_dataloader:\n","    \n","    mask = mask.squeeze()\n","    AnnMap = np.zeros(mask.shape,np.float32)\n","    AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","  \n","    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    #print(mask.shape)\n","    Pred = Net(images)['out'] # make prediction\n","\n","    # ---- CALCULANDO MÉTRICAS ----- #\n","    seg = torch.argmax(Pred,1).cpu().detach().numpy()  # Get  prediction classes\n","  \n","    # Accuracy\n","    accuracy = accuracy_score(AnnMap.flatten(), seg.flatten())\n","    # Precision\n","    precision_macro = precision_score(AnnMap.flatten(), seg.flatten(), average='macro')\n","    precision_micro = precision_score(AnnMap.flatten(), seg.flatten(), average='micro')\n","    # Recall:    \n","    recall = recall_score(AnnMap.flatten(), seg.flatten())\n","    # F1\n","    f1_macro = f1_score(AnnMap.flatten(), seg.flatten(), average='macro')\n","    f1_micro = f1_score(AnnMap.flatten(), seg.flatten(), average='micro')    \n","    \n","    # #print(f'Accuracy: {accuracy}, Precision_macro: {precision_macro}, Precision_micro: {precision_micro}, F1_macro: {f1_macro}, F1_micro: {f1_micro}, Recall: {recall}')\n","\n","\n","    test_metrics['accuracy'].append(accuracy)\n","    test_metrics['recall'].append(recall)\n","    test_metrics['precision_macro'].append(precision_macro)\n","    test_metrics['precision_micro'].append(precision_micro)      \n","    test_metrics['f1_macro'].append(f1_macro)\n","    test_metrics['f1_micro'].append(f1_micro)"],"metadata":{"id":"H1GJzWpm6OL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_results = pd.DataFrame(test_metrics)"],"metadata":{"id":"tHx24f5opBnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_results.mean(axis=0)"],"metadata":{"id":"XwNWSAw8pYQ5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8a6b7fdc261c4df196aaca3623fe72e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4943ada50864053a682a79df52e53fb","IPY_MODEL_8ca0b46d18f4491fae74df37f9aaf71c","IPY_MODEL_48b55f64b3e74f789faca6379fa7a888"],"layout":"IPY_MODEL_bbfde3b227014b2c8569183d938fb50b"}},"e4943ada50864053a682a79df52e53fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f719704deca24cd29f630e89681ef27e","placeholder":"​","style":"IPY_MODEL_0d7b8d95a3fc456bb70a5164c3432404","value":"100%"}},"8ca0b46d18f4491fae74df37f9aaf71c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a5119f0dca474b9add5d82bd1ab5fa","max":244545539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90de15d08366464893e71b806bce0584","value":244545539}},"48b55f64b3e74f789faca6379fa7a888":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_806e19abcc234c44a10347b765797aa2","placeholder":"​","style":"IPY_MODEL_b6f162f89e814469a253e338d32cfd0e","value":" 233M/233M [00:01&lt;00:00, 202MB/s]"}},"bbfde3b227014b2c8569183d938fb50b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f719704deca24cd29f630e89681ef27e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d7b8d95a3fc456bb70a5164c3432404":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4a5119f0dca474b9add5d82bd1ab5fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90de15d08366464893e71b806bce0584":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"806e19abcc234c44a10347b765797aa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6f162f89e814469a253e338d32cfd0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}