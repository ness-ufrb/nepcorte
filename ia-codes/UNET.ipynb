{"cells":[{"cell_type":"markdown","metadata":{"id":"yvvEPZP_zwa6"},"source":["**Possibilitando acesso a drive**"]},{"cell_type":"code","source":["!pip install segmentation-models-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Rvx6Ag1a4Tk","executionInfo":{"status":"ok","timestamp":1668019937394,"user_tz":180,"elapsed":2921,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"6eaefab1-3151-4a9c-e3fa-942989a1495b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.7.1)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.7.4)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.13.1+cu113)\n","Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.4.12)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (4.64.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12.1+cu113)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qHM8VHCjUGLR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668019940752,"user_tz":180,"elapsed":3362,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"a2f8abe3-4882-43ce-9449-38849e3534cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import data_table\n","data_table.enable_dataframe_formatter()"],"metadata":{"id":"-4dBY6sUFX7e","executionInfo":{"status":"ok","timestamp":1668019940752,"user_tz":180,"elapsed":6,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nabwFGiz5w5"},"source":["**Importando bibliotecas**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_7C4xX0NUzmL","executionInfo":{"status":"ok","timestamp":1668019940753,"user_tz":180,"elapsed":6,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["import os \n","import time\n","import numpy as np \n","import cv2 \n","import torchvision.models.segmentation \n","import torch \n","import torchvision.transforms as tf\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","#import segmentation_models_pytorch as smp\n","import torch.nn as nn\n","import time\n","import pandas as pd\n","import random\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","# Segmentation Model\n","#from torchvision.io.image import read_image\n","#from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n","#from torchvision.transforms.functional import to_pil_image\n","\n","# Função de Recall\n","from sklearn.metrics import recall_score\n","# Função de F1\n","from sklearn.metrics import f1_score\n","# Função de Precision\n","from sklearn.metrics import precision_score\n","# Função de Accuracy\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"Kp9Me-seaoqY"},"source":["**Inicializando constantes**"]},{"cell_type":"code","source":["import segmentation_models_pytorch as smp"],"metadata":{"id":"uk01w3pmaKs9","executionInfo":{"status":"ok","timestamp":1668021260546,"user_tz":180,"elapsed":587,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","execution_count":40,"metadata":{"id":"WqFtRtwoIZ8T","executionInfo":{"status":"ok","timestamp":1668021261506,"user_tz":180,"elapsed":3,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["model_name = 'UNET'\n","model_folder = f'/content/drive/MyDrive/DEV/saved-weights/{model_name}/'\n","\n","ImagesFolder=\"/content/drive/MyDrive/DEV/corte2/\" # ALTERAR ESSE CAMINHO PARA A PASTA CORTE 2 NA NUVEM COM GPUb"]},{"cell_type":"code","source":["n_epochs = 100\n","batchSize = 2 # Esse valor precisa ser divisor do número de imagens no treino\n","batchSize_val = 2 # Esse valor precisa ser divisor do número de imagens na validação\n","Learning_Rate = 1e-5"],"metadata":{"id":"iqvCNFiNmKMn","executionInfo":{"status":"ok","timestamp":1668021261507,"user_tz":180,"elapsed":3,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HY8LXMuAasic"},"source":["**Carregando arquivo com dataframe do dataset \"aleatoriezado\"**"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CwKoSvj_F4L4","outputId":"dfa38a7f-3208-4a8b-9a68-0bc1b8381b59","executionInfo":{"status":"ok","timestamp":1668021261992,"user_tz":180,"elapsed":2,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Train    640\n","Test      78\n","Val       78\n","Name: Status, dtype: int64"]},"metadata":{},"execution_count":42}],"source":["# Carregando dataframe de um arquivo chamado \"all_dataset_dataframe.csv\"\n","# Referencia https://towardsdatascience.com/how-to-read-csv-file-using-pandas-ab1f5e7e7b58\n","df = pd.read_csv(f'{ImagesFolder}all_dataset_dataframe.csv') # ALTERAR ESSE CAMINHO PARA A LOCALIZAÇÃO DO DATAFRAME NA NUVEM COM GPU\n","df['Status'].value_counts()\n"]},{"cell_type":"markdown","metadata":{"id":"ywxPjkHJbvi_"},"source":["**Lendo valores de desvio padrão e média de um arquivo**"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPZIgdYkmREx","outputId":"763c9bd7-f0a1-45fc-babf-92546a1ef81a","executionInfo":{"status":"ok","timestamp":1668021263311,"user_tz":180,"elapsed":4,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6604449458815983, 0.6988043731738397, 0.6452534234104293]\n","[0.1753238443181036, 0.19331901534376825, 0.1546044630633522]\n"]}],"source":["# Lendo a média e desvio padrão do arquivo \"mean_and_std_list.txt\"\n","mean_and_std_list_file = open(f\"{ImagesFolder}mean_and_std.txt\", \"r\")\n","content_list = mean_and_std_list_file.readlines()\n","\n","mean = [float(integer) for integer in content_list[1].split(' ')[0:3]]\n","std = [float(integer) for integer in content_list[3].split(' ')[0:3]]\n","\n","print(mean)\n","print(std)"]},{"cell_type":"markdown","metadata":{"id":"qnydFlHET-QS"},"source":["**Declaração do Dataset pelo Pytorch**"]},{"cell_type":"code","source":["def get_padding(w, h, max_w = 1056, max_h = 704):\n","  h_padding = (max_w - w) / 2\n","  v_padding = (max_h - h) / 2\n","  l_pad = h_padding if h_padding % 1 == 0 else h_padding+0.5\n","  t_pad = v_padding if v_padding % 1 == 0 else v_padding+0.5\n","  r_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n","  b_pad = v_padding if v_padding % 1 == 0 else v_padding-0.5\n","  \n","  padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n","  \n","  return padding"],"metadata":{"id":"nUdFBdq54PrI","executionInfo":{"status":"ok","timestamp":1668022549070,"user_tz":180,"elapsed":6,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["get_padding(w=1037, h=691)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rl3yky8q4Qn_","executionInfo":{"status":"ok","timestamp":1668022549070,"user_tz":180,"elapsed":5,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"236cda38-ddb4-4cdf-cf78-97a1a455bc4a"},"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 7, 9, 6)"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["class NewPad(object):\n","    def __init__(self, fill=0, padding_mode='constant'):       \n","        self.fill = fill\n","        self.padding_mode = padding_mode\n","        \n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (PIL Image): Image to be padded.\n","\n","        Returns:\n","            PIL Image: Padded image.\n","        \"\"\"\n","        return torchvision.transforms.functional.pad(img, (10, 7, 9, 6), self.fill, self.padding_mode)\n","    \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n","            format(self.fill, self.padding_mode)"],"metadata":{"id":"nM6Q9ZfB6gbB","executionInfo":{"status":"ok","timestamp":1668023024393,"user_tz":180,"elapsed":3,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":123,"outputs":[]},{"cell_type":"code","execution_count":131,"metadata":{"id":"sjrWa2An7FgK","executionInfo":{"status":"ok","timestamp":1668023064065,"user_tz":180,"elapsed":3,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["#UNET ERROR  Wrong input shape height=691, width=1037. Expected image height and width divisible by 32. Consider pad your images to shape (704, 1056).\n","def get_padding(w, h, max_w = 1056, max_h = 704):\n","  h_padding = (max_w - w) / 2\n","  v_padding = (max_h - h) / 2\n","  l_pad = h_padding if h_padding % 1 == 0 else h_padding+0.5\n","  t_pad = v_padding if v_padding % 1 == 0 else v_padding+0.5\n","  r_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n","  b_pad = v_padding if v_padding % 1 == 0 else v_padding-0.5\n","  \n","  padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n","  \n","  return padding\n","\n","# Referência: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","class CustomImageDataset(Dataset):\n","    \n","  def __init__(self, df):\n","    self.dataframe = df\n","    #----------------------------------------------Funções de transformação-------------------------------------------------------------------#\n","    self.transformImg=tf.Compose([tf.ToPILImage(), NewPad(), tf.ToTensor(),tf.Normalize((mean[0], mean[1], mean[2]), (std[0], std[1], std[2]))])\n","    self.transformMask=tf.Compose([tf.ToPILImage(), NewPad(), tf.ToTensor()])\n","\n","  def __len__(self):\n","    return len(self.dataframe)\n","\n","\n","  def __getitem__(self, idx):\n","    pair_path = os.path.join(ImagesFolder,  self.dataframe.iloc[idx].Path)\n","    Img = cv2.imread(os.path.join(pair_path, \"img.png\"))\n","    \n","        \n","    Label = cv2.imread(os.path.join(pair_path, \"label.png\"),0)\n","    \n","\n","    mask = np.zeros(Img.shape[0:2],np.float32)\n","    mask[Label != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","\n","    image = self.transformImg(Img)\n","    #image = torchvision.transforms.functional.pad(image, (10, 7, 9, 6))        \n","\n","    label = self.transformMask(mask)\n","    #label = torchvision.transforms.functional.pad(label, (10, 7, 9, 6))\n","\n","    return image, label"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"lrUgP-UOOQFf","executionInfo":{"status":"ok","timestamp":1668023064065,"user_tz":180,"elapsed":2,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["ds_train=CustomImageDataset(df[df['Status'] == 'Train'])\n","ds_val=CustomImageDataset(df[df['Status'] == 'Val'])"]},{"cell_type":"code","source":["print(len(ds_train))\n","print(len(ds_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fim9P8bay_Ga","executionInfo":{"status":"ok","timestamp":1668023064506,"user_tz":180,"elapsed":2,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"c6fd2d5f-15b5-4b14-ba3d-2ca9ddcecad7"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["640\n","78\n"]}]},{"cell_type":"markdown","metadata":{"id":"4Fa0qK2VULgJ"},"source":["**Instanciação do DataLoader's pelo Pytorch**"]},{"cell_type":"code","execution_count":134,"metadata":{"id":"Xi_buGBPQ1Jd","executionInfo":{"status":"ok","timestamp":1668023065125,"user_tz":180,"elapsed":2,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["train_dataloader = DataLoader(ds_train, batch_size=batchSize, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(ds_val, batch_size=batchSize_val, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","source":["# MODEL\n","\n"],"metadata":{"id":"0PLB8GVLNg7d"}},{"cell_type":"code","execution_count":135,"metadata":{"id":"_AwKg3JNPVX5","executionInfo":{"status":"ok","timestamp":1668023067516,"user_tz":180,"elapsed":1350,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"outputs":[],"source":["Net = smp.Unet(\n","    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=2,                      # model output channels (number of classes in your dataset)\n",")"]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","Net=Net.to(device)\n","optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"],"metadata":{"id":"t15joua4N7ut","executionInfo":{"status":"ok","timestamp":1668023067516,"user_tz":180,"elapsed":3,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":136,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftzhro928ZzG"},"source":["**Treinamento**"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"BxHqvnR-P5-A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668027940654,"user_tz":180,"elapsed":4671424,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"b9eeb2d7-ef49-43f0-9f99-7f5cfec866ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["training epoch: 0\n","it = 0, training_loss = 200.25007101893425, val_loss = 15.137700915336609, delta =  1.0\n","training epoch: 1\n","it = 1, training_loss = 104.55859637260437, val_loss = 10.98756954073906, delta =  0.27415863200156676\n","training epoch: 2\n","it = 2, training_loss = 79.25863309204578, val_loss = 8.57754448056221, delta =  0.21934105183508523\n","training epoch: 3\n","it = 3, training_loss = 63.40850067138672, val_loss = 7.0203061401844025, delta =  0.18154826756150377\n","training epoch: 4\n","it = 4, training_loss = 51.76329892873764, val_loss = 5.869944006204605, delta =  0.16386210387537048\n","training epoch: 5\n","it = 5, training_loss = 43.210013285279274, val_loss = 5.011394135653973, delta =  0.14626202049681125\n","training epoch: 6\n","it = 6, training_loss = 36.695907175540924, val_loss = 4.397205673158169, delta =  0.12255840308510357\n","training epoch: 7\n","it = 7, training_loss = 31.269465424120426, val_loss = 3.679548814892769, delta =  0.16320748029735965\n","training epoch: 8\n","it = 8, training_loss = 26.894470989704132, val_loss = 3.3575888350605965, delta =  0.08749985284311412\n","training epoch: 9\n","it = 9, training_loss = 23.161403596401215, val_loss = 2.932612732052803, delta =  0.12657181206052104\n","training epoch: 10\n","it = 10, training_loss = 20.028772987425327, val_loss = 2.549254074692726, delta =  0.13072256461620457\n","training epoch: 11\n","it = 11, training_loss = 17.309234220534563, val_loss = 2.320468381047249, delta =  0.08974613237523366\n","training epoch: 12\n","it = 12, training_loss = 14.955226313322783, val_loss = 2.0380486138164997, delta =  0.12170808684033452\n","training epoch: 13\n","it = 13, training_loss = 13.061304830014706, val_loss = 1.8641274385154247, delta =  0.08533710831135966\n","training epoch: 14\n","it = 14, training_loss = 11.146558096632361, val_loss = 1.6366985440254211, delta =  0.12200286836136376\n","training epoch: 15\n","it = 15, training_loss = 9.782688779756427, val_loss = 1.5291891619563103, delta =  0.06568673410357784\n","training epoch: 16\n","it = 16, training_loss = 8.552645768970251, val_loss = 1.4041636548936367, delta =  0.08175934683105324\n","training epoch: 17\n","it = 17, training_loss = 7.602853396907449, val_loss = 1.3356510922312737, delta =  0.04879243414654022\n","training epoch: 18\n","it = 18, training_loss = 6.954190071672201, val_loss = 1.2293756026774645, delta =  0.0795683020602862\n","training epoch: 19\n","it = 19, training_loss = 6.053347604349256, val_loss = 1.1911295372992754, delta =  0.031110154858200123\n","training epoch: 20\n","it = 20, training_loss = 5.399265797808766, val_loss = 1.1366442888975143, delta =  0.04574250465259977\n","training epoch: 21\n","it = 21, training_loss = 4.934121750295162, val_loss = 1.1354203671216965, delta =  0.0010767852245182619\n","training epoch: 22\n","it = 22, training_loss = 4.43172809202224, val_loss = 1.0779410358518362, delta =  0.051646107422593235\n","training epoch: 23\n","it = 23, training_loss = 4.032379614189267, val_loss = 1.047387887723744, delta =  0.028343988318385027\n","training epoch: 24\n","it = 24, training_loss = 3.6850459314882755, val_loss = 1.0011661406606436, delta =  0.04413049607013564\n","training epoch: 25\n","it = 25, training_loss = 3.3900594422593713, val_loss = 0.9884542729705572, delta =  0.012697061130831022\n","training epoch: 26\n","it = 26, training_loss = 3.0919183026999235, val_loss = 0.9671112690120935, delta =  0.021592302792442286\n","training epoch: 27\n","it = 27, training_loss = 2.865066912956536, val_loss = 0.9393969122320414, delta =  0.028656844013783878\n","training epoch: 28\n","it = 28, training_loss = 2.6867540664970875, val_loss = 0.8964880546554923, delta =  0.045677026417508704\n","training epoch: 29\n","it = 29, training_loss = 2.455821667332202, val_loss = 0.9252355080097914, delta =  -0.0320667444535514\n","training epoch: 30\n","it = 30, training_loss = 2.3069658409804106, val_loss = 0.9629561817273498, delta =  -0.07414279167099469\n","training epoch: 31\n","it = 31, training_loss = 2.1180249634198844, val_loss = 0.9628219259902835, delta =  -0.07399303425217685\n","training epoch: 32\n","it = 32, training_loss = 2.0272580324672163, val_loss = 0.9646050566807389, delta =  -0.07598205204354125\n","training epoch: 33\n"]}],"source":["epoch = []\n","\n","train_loss = []\n","train_loss_sum = 0\n","\n","val_loss = []\n","val_loss_sum = 0\n","\n","cont2 = 0\n","\n","val_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n","               'f1_macro' : [], 'f1_micro' : [], 'training_loss' : [], 'validation_loss' : []}\n","loss_dict = {'training' : [], 'validation' : []} \n","\n","train_prev_loss = float('-inf')\n","train_best_loss = float('inf')\n","train_last_improvement = 0\n","\n","\n","val_prev_loss = float('-inf')\n","val_best_loss = float('inf')\n","val_last_improvement = 0\n","\n","patience_n_iterations = 5\n","patience_min_threshold = 0.01\n","\n","\n","#Net.load_state_dict(torch.load(\"0.torch\")) # Load trained model\n","start = time.time()\n","for itr in range(n_epochs): # Training loop\n","  print(f'training epoch: {itr}')\n","  batch_it = 0\n","  train_loss_sum = 0\n","  \n","  #TRAINING\n","  Net.train()\n","  for images, mask in train_dataloader:\n","    Net.zero_grad()\n","    \n","    # Iterar sobre o dataloader do treino    \n","    mask = mask.squeeze()\n","    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    \n","    Pred=Net(images) # make prediction\n","    \n","    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n","    \n","    Loss = criterion(Pred,mask.long()) # Calculate cross entropy loss    \n","    Loss.backward() # Backpropogate loss\n","    optimizer.step() # Apply gradient descent change to weight\n","    \n","    #print(f'Training Batch Iteration = {batch_it}, Training Loss = {Loss.data.cpu().numpy()}')\n","    train_loss_sum += Loss.data.cpu().numpy()\n","    batch_it += 1\n","    \n","  delta = 1 - train_loss_sum / train_prev_loss\n","  if(delta >= patience_min_threshold):\n","    train_prev_loss = train_loss_sum\n","    train_last_improvement = 0  \n","  else:\n","    train_last_improvement += 1\n","    if train_last_improvement >= patience_n_iterations:\n","        break\n","  \n","  train_loss.append(train_loss_sum/len(train_dataloader))\n","\n","  #VALIDATION \n","  val_loss_sum = 0\n","\n","  Net.eval()\n","  with torch.no_grad():\n","        \n","    for images, mask in val_dataloader:\n","        \n","      mask = mask.squeeze()\n","      AnnMap = np.zeros(mask.shape,np.float32)\n","      AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","    \n","      images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","      mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    \n","      Pred = Net(images) # make prediction     \n","\n","      criterion = torch.nn.CrossEntropyLoss() # Set loss function\n","      Loss=criterion(Pred,mask.long()) # Calculate cross entropy loss      \n","    \n","      val_loss_sum += Loss.data.cpu().numpy()\n","      cont2+=1\n","\n","\n","  delta = 1 - val_loss_sum / val_prev_loss\n","  if(delta >= patience_min_threshold):\n","    val_prev_loss = val_loss_sum\n","    val_last_improvement = 0  \n","  else:\n","    val_last_improvement += 1\n","    if val_last_improvement >= patience_n_iterations:\n","        break  \n","\n","  if(val_loss_sum < val_best_loss ):\n","    val_best_loss  = val_loss_sum\n","    model_weight_name = f'{model_name}_best_{Learning_Rate}'\n","    torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n","  \n","  val_loss.append(val_loss_sum/len(val_dataloader))\n","\n","  # Salvando perdas em dataframe\n","  loss_dict['training'].append(train_loss_sum)\n","  loss_dict['validation'].append(val_loss_sum)\n","\n","  # Salvando pesos\n","  model_weight_name = f'{model_name}_{Learning_Rate}'  \n","  torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n","\n","  print(f'it = {itr}, training_loss = {train_loss_sum}, val_loss = {val_loss_sum}, delta =  {delta}')\n","# ---- FINALIZANDO TREINO ----- #\n","end = time.time()"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"x2V0kyvcxY2h","executionInfo":{"status":"ok","timestamp":1668028011869,"user_tz":180,"elapsed":71231,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"fc4ae910-38a0-480e-bd64-1b6e568350db"},"outputs":[{"output_type":"stream","name":"stdout","text":["01:17:51 com 100 épocas\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Salvando dataframe de métricas\n","df_metrics = pd.DataFrame(val_metrics)\n","df_metrics.to_csv('dataframe_metrics.csv', index=False)\n","\n","# Duração do treino e validação\n","trainingDuration = time.strftime(\"%H:%M:%S\", time.gmtime((end-start)))\n","print(f\"{trainingDuration} com {n_epochs} épocas\")\n","\n","# Plot de gráfico loss do treino por épocas de treinamento\n","plt.title(\"Model Loss\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","\n","for i in range(n_epochs):\n","  plt.plot(train_loss, color=\"b\", label=\"train\")\n","  plt.plot(val_loss, color=\"g\", label=\"validation\")\n","  plt.legend([\"train\",\"validation\"])"]},{"cell_type":"markdown","source":["# TESTING"],"metadata":{"id":"VxIwXQq45_4C"}},{"cell_type":"code","source":["model_path = f'{model_folder}{model_name}_best_{Learning_Rate}.torch'\n","\n","Net = smp.Unet(\n","    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=2,                      # model output channels (number of classes in your dataset)\n",")\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","Net = Net.to(device)  # Set net to GPU or CPU\n","Net.load_state_dict(torch.load(model_path,  map_location=device)) # Load trained model"],"metadata":{"id":"68ci_Fiwz0sB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668028012341,"user_tz":180,"elapsed":480,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"e7f9c02c-315c-4690-da3a-5fed2bf338bb"},"execution_count":144,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Unet(\n","  (encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (decoder): UnetDecoder(\n","    (center): Identity()\n","    (blocks): ModuleList(\n","      (0): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (1): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (2): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (4): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (segmentation_head): SegmentationHead(\n","    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Identity()\n","    (2): Activation(\n","      (activation): Identity()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["test_dataloader = DataLoader(CustomImageDataset(df[df['Status'] == 'Test']), batch_size=1, shuffle=False)"],"metadata":{"id":"hEH5xMxO6Cfy","executionInfo":{"status":"ok","timestamp":1668028012341,"user_tz":180,"elapsed":6,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":["test_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n","               'f1_macro' : [], 'f1_micro' : []}"],"metadata":{"id":"WuaaRDqPo7ml","executionInfo":{"status":"ok","timestamp":1668028012341,"user_tz":180,"elapsed":5,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["Net.eval()\n","with torch.no_grad():\n","  # Iterar sobre o dataloader da validação, configurar o batchsize por questões de memória\n","  for images, mask in test_dataloader:\n","    \n","    mask = mask.squeeze()\n","    AnnMap = np.zeros(mask.shape,np.float32)\n","    AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n","  \n","    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n","    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n","    #print(mask.shape)\n","    Pred = Net(images) # make prediction\n","\n","    # ---- CALCULANDO MÉTRICAS ----- #\n","    seg = torch.argmax(Pred,1).cpu().detach().numpy()  # Get  prediction classes\n","  \n","    # Accuracy\n","    accuracy = accuracy_score(AnnMap.flatten(), seg.flatten())\n","    # Precision\n","    precision_macro = precision_score(AnnMap.flatten(), seg.flatten(), average='macro')\n","    precision_micro = precision_score(AnnMap.flatten(), seg.flatten(), average='micro')\n","    # Recall:    \n","    recall = recall_score(AnnMap.flatten(), seg.flatten())\n","    # F1\n","    f1_macro = f1_score(AnnMap.flatten(), seg.flatten(), average='macro')\n","    f1_micro = f1_score(AnnMap.flatten(), seg.flatten(), average='micro')    \n","    \n","    # #print(f'Accuracy: {accuracy}, Precision_macro: {precision_macro}, Precision_micro: {precision_micro}, F1_macro: {f1_macro}, F1_micro: {f1_micro}, Recall: {recall}')\n","\n","\n","    test_metrics['accuracy'].append(accuracy)\n","    test_metrics['recall'].append(recall)\n","    test_metrics['precision_macro'].append(precision_macro)\n","    test_metrics['precision_micro'].append(precision_micro)      \n","    test_metrics['f1_macro'].append(f1_macro)\n","    test_metrics['f1_micro'].append(f1_micro)"],"metadata":{"id":"H1GJzWpm6OL5","executionInfo":{"status":"ok","timestamp":1668028334761,"user_tz":180,"elapsed":322424,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":147,"outputs":[]},{"cell_type":"code","source":["test_results = pd.DataFrame(test_metrics)"],"metadata":{"id":"tHx24f5opBnU","executionInfo":{"status":"ok","timestamp":1668028334761,"user_tz":180,"elapsed":27,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["test_results.mean(axis=0)"],"metadata":{"id":"XwNWSAw8pYQ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668028334761,"user_tz":180,"elapsed":25,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"f8aced93-7fa9-4591-bddd-1794ca89d3ff"},"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["accuracy           0.994281\n","precision_macro    0.982407\n","precision_micro    0.994281\n","recall             0.957787\n","f1_macro           0.977943\n","f1_micro           0.994281\n","dtype: float64"]},"metadata":{},"execution_count":149}]},{"cell_type":"code","source":["end-start"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tj3v5AePeai","executionInfo":{"status":"ok","timestamp":1668028429966,"user_tz":180,"elapsed":4,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"b8344cdf-9fd6-4a91-b4fa-dac10a4948d5"},"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4671.006484508514"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["(end-start)/3600"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKqT8YimPfAS","executionInfo":{"status":"ok","timestamp":1668028445752,"user_tz":180,"elapsed":2,"user":{"displayName":"NEP Corte","userId":"14883523365544643318"}},"outputId":"c3293772-ebe5-4619-adcd-6383858d7ae6"},"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.2975018012523651"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":[],"metadata":{"id":"Msk8jRBfPi2q"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1C_miXtm4kCKYst3GjKD8q0SAzMRAjBfa","timestamp":1668008219272},{"file_id":"1GexIy3r705iVm_ErN3cQdveq3x0Sk6Fd","timestamp":1667994385740}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}