{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvEPZP_zwa6"
      },
      "source": [
        "**Possibilitando acesso a drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qHM8VHCjUGLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26a401b-2380-4edf-8dfb-353c39d4033b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "-4dBY6sUFX7e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nabwFGiz5w5"
      },
      "source": [
        "**Importando bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_7C4xX0NUzmL"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import time\n",
        "import numpy as np \n",
        "import cv2 \n",
        "import torchvision.models.segmentation \n",
        "import torch \n",
        "import torchvision.transforms as tf\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "#import segmentation_models_pytorch as smp\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Segmentation Model\n",
        "#from torchvision.io.image import read_image\n",
        "#from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
        "#from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Função de Recall\n",
        "from sklearn.metrics import recall_score\n",
        "# Função de F1\n",
        "from sklearn.metrics import f1_score\n",
        "# Função de Precision\n",
        "from sklearn.metrics import precision_score\n",
        "# Função de Accuracy\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp9Me-seaoqY"
      },
      "source": [
        "**Inicializando constantes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WqFtRtwoIZ8T"
      },
      "outputs": [],
      "source": [
        "model_name = 'FCN_RESNET101'\n",
        "model_folder = f'/content/drive/MyDrive/projeto-ia/DEV/saved-weights/{model_name}/'\n",
        "\n",
        "ImagesFolder=\"/content/drive/MyDrive/projeto-ia/DEV/corte2/\" # ALTERAR ESSE CAMINHO PARA A PASTA CORTE 2 NA NUVEM COM GPUb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "batchSize = 2 # Esse valor precisa ser divisor do número de imagens no treino\n",
        "batchSize_val = 2 # Esse valor precisa ser divisor do número de imagens na validação\n",
        "Learning_Rate = 1e-5"
      ],
      "metadata": {
        "id": "iqvCNFiNmKMn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY8LXMuAasic"
      },
      "source": [
        "**Carregando arquivo com dataframe do dataset \"aleatoriezado\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwKoSvj_F4L4",
        "outputId": "222f7fce-59a3-4ca6-f6eb-f79c02c62c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Train    640\n",
              "Test      78\n",
              "Val       78\n",
              "Name: Status, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Carregando dataframe de um arquivo chamado \"all_dataset_dataframe.csv\"\n",
        "# Referencia https://towardsdatascience.com/how-to-read-csv-file-using-pandas-ab1f5e7e7b58\n",
        "df = pd.read_csv(f'{ImagesFolder}all_dataset_dataframe.csv') # ALTERAR ESSE CAMINHO PARA A LOCALIZAÇÃO DO DATAFRAME NA NUVEM COM GPU\n",
        "df['Status'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywxPjkHJbvi_"
      },
      "source": [
        "**Lendo valores de desvio padrão e média de um arquivo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPZIgdYkmREx",
        "outputId": "4c75e69e-b00f-44ab-cc1a-9dee036274a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6604449458815983, 0.6988043731738397, 0.6452534234104293]\n",
            "[0.1753238443181036, 0.19331901534376825, 0.1546044630633522]\n"
          ]
        }
      ],
      "source": [
        "# Lendo a média e desvio padrão do arquivo \"mean_and_std_list.txt\"\n",
        "mean_and_std_list_file = open(f\"{ImagesFolder}mean_and_std.txt\", \"r\")\n",
        "content_list = mean_and_std_list_file.readlines()\n",
        "\n",
        "mean = [float(integer) for integer in content_list[1].split(' ')[0:3]]\n",
        "std = [float(integer) for integer in content_list[3].split(' ')[0:3]]\n",
        "\n",
        "print(mean)\n",
        "print(std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnydFlHET-QS"
      },
      "source": [
        "**Declaração do Dataset pelo Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sjrWa2An7FgK"
      },
      "outputs": [],
      "source": [
        "# Referência: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "class CustomImageDataset(Dataset):\n",
        "    \n",
        "  def __init__(self, df):\n",
        "    self.dataframe = df\n",
        "    #----------------------------------------------Funções de transformação-------------------------------------------------------------------#\n",
        "    self.transformImg=tf.Compose([tf.ToPILImage(), tf.ToTensor(),tf.Normalize((mean[0], mean[1], mean[2]), (std[0], std[1], std[2]))])\n",
        "    self.transformMask=tf.Compose([tf.ToPILImage(), tf.ToTensor()])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pair_path = os.path.join(ImagesFolder,  self.dataframe.iloc[idx].Path)\n",
        "    Img = cv2.imread(os.path.join(pair_path, \"img.png\"))\n",
        "    Label = cv2.imread(os.path.join(pair_path, \"label.png\"),0)\n",
        "\n",
        "    mask = np.zeros(Img.shape[0:2],np.float32)\n",
        "    mask[Label != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n",
        "\n",
        "    image = self.transformImg(Img)\n",
        "    label = self.transformMask(mask)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lrUgP-UOOQFf"
      },
      "outputs": [],
      "source": [
        "ds_train=CustomImageDataset(df[df['Status'] == 'Train'])\n",
        "ds_val=CustomImageDataset(df[df['Status'] == 'Val'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds_train))\n",
        "print(len(ds_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fim9P8bay_Ga",
        "outputId": "b31876db-a602-4213-f351-73b9769dea75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640\n",
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fa0qK2VULgJ"
      },
      "source": [
        "**Instanciação do DataLoader's pelo Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xi_buGBPQ1Jd"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(ds_train, batch_size=batchSize, shuffle=True)\n",
        "val_dataloader = DataLoader(ds_val, batch_size=batchSize_val, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK6z-HMjUclc"
      },
      "source": [
        " **Definição da estrutura FCN_RESNET101**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsaE6vAic3wN"
      },
      "source": [
        " Referência: https://pytorch.org/vision/stable/models/generated/torchvision.models.segmentation.fcn_resnet101.html#torchvision.models.segmentation.FCN_ResNet101_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "0PLB8GVLNg7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "de6397056f23449e8b9b00c5a3876ca5",
            "c6891c19dc954325b8bc72cc24b7bf81",
            "29c5df8c137c40ea8659f183beadf574",
            "83da69eddcf5435baa9142ac1c26c60c",
            "f7b61218f0c64fa2b0423c150ebf840f",
            "786c5a9ab94743329c7b385be4c21fd2",
            "ba86cdd90b5a48a49368f8c91fa9584d",
            "76370ed0d9e641a5ac1a3749b396e483",
            "6f61dacb22394558ab5ba0486d1675da",
            "3340a5c780f144e39f95210afc758a2f",
            "68ee3de8afd947cb8007e95ac214f9e7"
          ]
        },
        "id": "_AwKg3JNPVX5",
        "outputId": "80e85b6a-b366-441a-d501-dd2388d24ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet101_coco-7ecb50ca.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/208M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de6397056f23449e8b9b00c5a3876ca5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#--------------Load and set net and optimizer-------------------------------------\n",
        "Net = torchvision.models.segmentation.fcn_resnet101(pretrained=True) # Load net\n",
        "Net.classifier[4] = torch.nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "Net=Net.to(device)\n",
        "optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"
      ],
      "metadata": {
        "id": "dfbE59OiNzRV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftzhro928ZzG"
      },
      "source": [
        "**Treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BxHqvnR-P5-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76cdb97-55e5-4f1b-a435-4110fd9d251c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training epoch: 0\n",
            "it = 0, training_loss = 35.29047164320946, val_loss = 3.680041089653969, delta =  1.0\n",
            "training epoch: 1\n",
            "it = 1, training_loss = 26.09348025918007, val_loss = 2.9018071219325066, delta =  0.211474260412304\n",
            "training epoch: 2\n",
            "it = 2, training_loss = 20.200223311781883, val_loss = 2.4885017164051533, delta =  0.14243035052312703\n",
            "training epoch: 3\n",
            "it = 3, training_loss = 16.00041626021266, val_loss = 1.9333376958966255, delta =  0.2230916767501805\n",
            "training epoch: 4\n",
            "it = 4, training_loss = 12.818928491324186, val_loss = 1.5804729983210564, delta =  0.18251581103730607\n",
            "training epoch: 5\n",
            "it = 5, training_loss = 10.527031360194087, val_loss = 1.4029914122074842, delta =  0.11229650003645208\n",
            "training epoch: 6\n",
            "it = 6, training_loss = 8.754062937572598, val_loss = 1.2458565086126328, delta =  0.11199990408181715\n",
            "training epoch: 7\n",
            "it = 7, training_loss = 8.017188789322972, val_loss = 1.1462891846895218, delta =  0.0799187733376997\n",
            "training epoch: 8\n",
            "it = 8, training_loss = 6.390522848814726, val_loss = 1.057193037122488, delta =  0.07772571595113309\n",
            "training epoch: 9\n",
            "it = 9, training_loss = 5.49344102665782, val_loss = 0.9718822576105595, delta =  0.08069555560461406\n",
            "training epoch: 10\n",
            "it = 10, training_loss = 4.887035164050758, val_loss = 0.9134637964889407, delta =  0.06010857865153807\n",
            "training epoch: 11\n",
            "it = 11, training_loss = 4.144284270703793, val_loss = 0.9634690741077065, delta =  -0.054742484388510926\n",
            "training epoch: 12\n",
            "it = 12, training_loss = 3.6212239675223827, val_loss = 0.8579748896881938, delta =  0.06074560044309185\n",
            "training epoch: 13\n",
            "it = 13, training_loss = 3.2128594694659114, val_loss = 0.8158318558707833, delta =  0.049119192559034186\n",
            "training epoch: 14\n",
            "it = 14, training_loss = 2.8648346825502813, val_loss = 0.8087948020547628, delta =  0.008625617846841016\n",
            "training epoch: 15\n",
            "it = 15, training_loss = 2.5574747351929545, val_loss = 0.8285245392471552, delta =  -0.015557964898078369\n",
            "training epoch: 16\n",
            "it = 16, training_loss = 2.332864078693092, val_loss = 0.8173376815393567, delta =  -0.0018457549282211438\n",
            "training epoch: 17\n",
            "it = 17, training_loss = 2.100457346998155, val_loss = 0.848647310398519, delta =  -0.04022330617711645\n",
            "training epoch: 18\n"
          ]
        }
      ],
      "source": [
        "epoch = []\n",
        "\n",
        "train_loss = []\n",
        "train_loss_sum = 0\n",
        "\n",
        "val_loss = []\n",
        "val_loss_sum = 0\n",
        "\n",
        "cont2 = 0\n",
        "\n",
        "val_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n",
        "               'f1_macro' : [], 'f1_micro' : [], 'training_loss' : [], 'validation_loss' : []}\n",
        "loss_dict = {'training' : [], 'validation' : []} \n",
        "\n",
        "train_prev_loss = float('-inf')\n",
        "train_best_loss = float('inf')\n",
        "train_last_improvement = 0\n",
        "\n",
        "\n",
        "val_prev_loss = float('-inf')\n",
        "val_best_loss = float('inf')\n",
        "val_last_improvement = 0\n",
        "\n",
        "patience_n_iterations = 5\n",
        "patience_min_threshold = 0.01\n",
        "\n",
        "\n",
        "#Net.load_state_dict(torch.load(\"0.torch\")) # Load trained model\n",
        "start = time.time()\n",
        "for itr in range(n_epochs): # Training loop\n",
        "  print(f'training epoch: {itr}')\n",
        "  batch_it = 0\n",
        "  train_loss_sum = 0\n",
        "  \n",
        "  #TRAINING\n",
        "  Net.train()\n",
        "  for images, mask in train_dataloader:\n",
        "    Net.zero_grad()\n",
        "    \n",
        "    # Iterar sobre o dataloader do treino    \n",
        "    mask = mask.squeeze()\n",
        "    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n",
        "    \n",
        "    Pred=Net(images)['out'] # make prediction\n",
        "    \n",
        "    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
        "    \n",
        "    Loss = criterion(Pred,mask.long()) # Calculate cross entropy loss    \n",
        "    Loss.backward() # Backpropogate loss\n",
        "    optimizer.step() # Apply gradient descent change to weight\n",
        "    \n",
        "    #print(f'Training Batch Iteration = {batch_it}, Training Loss = {Loss.data.cpu().numpy()}')\n",
        "    train_loss_sum += Loss.data.cpu().numpy()\n",
        "    batch_it += 1\n",
        "    \n",
        "  delta = 1 - train_loss_sum / train_prev_loss\n",
        "  if(delta >= patience_min_threshold):\n",
        "    train_prev_loss = train_loss_sum\n",
        "    train_last_improvement = 0  \n",
        "  else:\n",
        "    train_last_improvement += 1\n",
        "    if train_last_improvement >= patience_n_iterations:\n",
        "        break\n",
        "  \n",
        "  train_loss.append(train_loss_sum/len(train_dataloader))\n",
        "\n",
        "  #VALIDATION \n",
        "  val_loss_sum = 0\n",
        "\n",
        "  Net.eval()\n",
        "  with torch.no_grad():\n",
        "        \n",
        "    for images, mask in val_dataloader:\n",
        "        \n",
        "      mask = mask.squeeze()\n",
        "      AnnMap = np.zeros(mask.shape,np.float32)\n",
        "      AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n",
        "    \n",
        "      images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "      mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n",
        "    \n",
        "      Pred = Net(images)['out'] # make prediction     \n",
        "\n",
        "      criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
        "      Loss=criterion(Pred,mask.long()) # Calculate cross entropy loss      \n",
        "    \n",
        "      val_loss_sum += Loss.data.cpu().numpy()\n",
        "      cont2+=1\n",
        "\n",
        "\n",
        "  delta = 1 - val_loss_sum / val_prev_loss\n",
        "  if(delta >= patience_min_threshold):\n",
        "    val_prev_loss = val_loss_sum\n",
        "    val_last_improvement = 0  \n",
        "  else:\n",
        "    val_last_improvement += 1\n",
        "    if val_last_improvement >= patience_n_iterations:\n",
        "        break  \n",
        "\n",
        "  if(val_loss_sum < val_best_loss ):\n",
        "    val_best_loss  = val_loss_sum\n",
        "    model_weight_name = f'{model_name}_best_{Learning_Rate}'\n",
        "    torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n",
        "  \n",
        "  val_loss.append(val_loss_sum/len(val_dataloader))\n",
        "\n",
        "  # Salvando perdas em dataframe\n",
        "  loss_dict['training'].append(train_loss_sum)\n",
        "  loss_dict['validation'].append(val_loss_sum)\n",
        "\n",
        "  # Salvando pesos\n",
        "  model_weight_name = f'{model_name}_{Learning_Rate}'  \n",
        "  torch.save(Net.state_dict(), f\"{model_folder}{model_weight_name}.torch\") \n",
        "\n",
        "  print(f'it = {itr}, training_loss = {train_loss_sum}, val_loss = {val_loss_sum}, delta =  {delta}')\n",
        "# ---- FINALIZANDO TREINO ----- #\n",
        "end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x2V0kyvcxY2h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "419c122e-d8b5-4225-832f-924429e3fea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04:17:58 com 100 épocas\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Salvando dataframe de métricas\n",
        "df_metrics = pd.DataFrame(val_metrics)\n",
        "df_metrics.to_csv('dataframe_metrics.csv', index=False)\n",
        "\n",
        "# Salvando dataframe de losses\n",
        "df_metrics = pd.DataFrame(loss_dict)\n",
        "df_metrics.to_csv('dataframe_losses.csv', index=False)\n",
        "\n",
        "# Duração do treino e validação\n",
        "trainingDuration = time.strftime(\"%H:%M:%S\", time.gmtime((end-start)))\n",
        "print(f\"{trainingDuration} com {n_epochs} épocas\")\n",
        "\n",
        "# Plot de gráfico loss do treino por épocas de treinamento\n",
        "plt.title(\"Model Loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "for i in range(n_epochs):\n",
        "  plt.plot(train_loss, color=\"b\", label=\"train\")\n",
        "  plt.plot(val_loss, color=\"g\", label=\"validation\")\n",
        "  plt.legend([\"train\",\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "VxIwXQq45_4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{model_folder}{model_name}_best_{Learning_Rate}.torch'\n",
        "\n",
        "Net = torchvision.models.segmentation.fcn_resnet101(pretrained=True)  # Load net\n",
        "Net.classifier[4] = torch.nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))  # Change final layer to 3 classes\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "Net = Net.to(device)  # Set net to GPU or CPU\n",
        "Net.load_state_dict(torch.load(model_path,  map_location=device)) # Load trained model\n",
        "Net.eval() # Set to evaluation mode"
      ],
      "metadata": {
        "id": "68ci_Fiwz0sB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6daab6-6ae1-475d-cdfa-f4e0e5738146"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FCN(\n",
              "  (backbone): IntermediateLayerGetter(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (8): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (9): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (10): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (11): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (12): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (13): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (14): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (15): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (16): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (17): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (18): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (19): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (20): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (21): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (22): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): FCNHead(\n",
              "    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux_classifier): FCNHead(\n",
              "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(CustomImageDataset(df[df['Status'] == 'Test']), batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "hEH5xMxO6Cfy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = {'accuracy': [], 'precision_macro' : [], 'precision_micro' : [], 'recall' : [], \n",
        "               'f1_macro' : [], 'f1_micro' : []}"
      ],
      "metadata": {
        "id": "WuaaRDqPo7ml"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Net.eval()\n",
        "with torch.no_grad():\n",
        "  # Iterar sobre o dataloader da validação, configurar o batchsize por questões de memória\n",
        "  for images, mask in test_dataloader:\n",
        "    \n",
        "    mask = mask.squeeze()\n",
        "    AnnMap = np.zeros(mask.shape,np.float32)\n",
        "    AnnMap[mask != 0] = 1 # set to 1 only the pixels that corresponds to the mask\n",
        "  \n",
        "    images = torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "    mask = torch.autograd.Variable(mask, requires_grad=False).to(device) # Load annotation\n",
        "    #print(mask.shape)\n",
        "    Pred = Net(images)['out'] # make prediction\n",
        "\n",
        "    # ---- CALCULANDO MÉTRICAS ----- #\n",
        "    seg = torch.argmax(Pred,1).cpu().detach().numpy()  # Get  prediction classes\n",
        "  \n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(AnnMap.flatten(), seg.flatten())\n",
        "    # Precision\n",
        "    precision_macro = precision_score(AnnMap.flatten(), seg.flatten(), average='macro')\n",
        "    precision_micro = precision_score(AnnMap.flatten(), seg.flatten(), average='micro')\n",
        "    # Recall:    \n",
        "    recall = recall_score(AnnMap.flatten(), seg.flatten())\n",
        "    # F1\n",
        "    f1_macro = f1_score(AnnMap.flatten(), seg.flatten(), average='macro')\n",
        "    f1_micro = f1_score(AnnMap.flatten(), seg.flatten(), average='micro')    \n",
        "    \n",
        "    # #print(f'Accuracy: {accuracy}, Precision_macro: {precision_macro}, Precision_micro: {precision_micro}, F1_macro: {f1_macro}, F1_micro: {f1_micro}, Recall: {recall}')\n",
        "\n",
        "\n",
        "    test_metrics['accuracy'].append(accuracy)\n",
        "    test_metrics['recall'].append(recall)\n",
        "    test_metrics['precision_macro'].append(precision_macro)\n",
        "    test_metrics['precision_micro'].append(precision_micro)      \n",
        "    test_metrics['f1_macro'].append(f1_macro)\n",
        "    test_metrics['f1_micro'].append(f1_micro)"
      ],
      "metadata": {
        "id": "H1GJzWpm6OL5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = pd.DataFrame(test_metrics)"
      ],
      "metadata": {
        "id": "tHx24f5opBnU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results.mean(axis=0)"
      ],
      "metadata": {
        "id": "XwNWSAw8pYQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6879cd28-5fe4-4212-c5d7-50f82a3855be"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "accuracy           0.994817\n",
              "precision_macro    0.984310\n",
              "precision_micro    0.994817\n",
              "recall             0.965555\n",
              "f1_macro           0.982339\n",
              "f1_micro           0.994817\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de6397056f23449e8b9b00c5a3876ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6891c19dc954325b8bc72cc24b7bf81",
              "IPY_MODEL_29c5df8c137c40ea8659f183beadf574",
              "IPY_MODEL_83da69eddcf5435baa9142ac1c26c60c"
            ],
            "layout": "IPY_MODEL_f7b61218f0c64fa2b0423c150ebf840f"
          }
        },
        "c6891c19dc954325b8bc72cc24b7bf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_786c5a9ab94743329c7b385be4c21fd2",
            "placeholder": "​",
            "style": "IPY_MODEL_ba86cdd90b5a48a49368f8c91fa9584d",
            "value": "100%"
          }
        },
        "29c5df8c137c40ea8659f183beadf574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76370ed0d9e641a5ac1a3749b396e483",
            "max": 217800805,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f61dacb22394558ab5ba0486d1675da",
            "value": 217800805
          }
        },
        "83da69eddcf5435baa9142ac1c26c60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3340a5c780f144e39f95210afc758a2f",
            "placeholder": "​",
            "style": "IPY_MODEL_68ee3de8afd947cb8007e95ac214f9e7",
            "value": " 208M/208M [00:02&lt;00:00, 81.9MB/s]"
          }
        },
        "f7b61218f0c64fa2b0423c150ebf840f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786c5a9ab94743329c7b385be4c21fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba86cdd90b5a48a49368f8c91fa9584d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76370ed0d9e641a5ac1a3749b396e483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f61dacb22394558ab5ba0486d1675da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3340a5c780f144e39f95210afc758a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ee3de8afd947cb8007e95ac214f9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}